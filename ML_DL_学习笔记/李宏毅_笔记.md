# 01-Regression

##  Machine Learning

> 概况：机器学习就是让机器具备找一个函式的能力。

##  三个机器学习任务

- Regression：要找的函式,他的输出是一个数值
- Classification:函式的输出,就是从设定好的选项裡面,选择一个当作输出
- Structured Learning:机器产生有结构的东西的问题——学会创造

## 找函式的过程：三个步骤

1. 带有未知参数的函式

   - 带有Unknown的Parameter的Function => model

2. 定义一个东西叫做Loss(损失函数)

   - Loss它也是一个Function,那这个Function它的输入,是 我们Model裡面的参数
     - L越大,代表一组参数越不好,这个大L越小,代表现在这一组参数越好
     - 计算方法：求取估测的值跟实际的值（Label） 之间的差距
       - MAE(mean absolute error)——平均绝对误差
       - MSE(mean square error)——均方误差
       - Cross-entropy:计算**概率分布**之间的差距——交叉熵

3. Optimization(优化)

   - 找到能让损失函数值最小的参数

   - 具体方法：Gradient Descent（梯度下降）

     1. 随机选取初始值 $w_0$

     2. 计算在 $w=w_0$的时候,*w*这个参数对*loss*的微分是多少

     3. 根据微分（梯度）的方向，改变参数的值

        - **改变的大小取决于：**
          1. 斜率的大小
          2. 学习率的大小**（超参数）**

     4. 什么时候停下来？

        - 自己设置上限**（超参数）**

        - 理想情况：微分值为0（极小值点），不会再更新⇒有可能陷入局部最小值，不能找到全局最小值

          **事实上：局部最小值不是真正的问题！！！**

##  Liner Model（线性模型）

==Sigmoid函数==

```latex
Sigmoid：y = c\frac{1}{{1+e^{-(b+wx_1)}}}
```

​	

<img src="https://diamond-mule-bee.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F18d619c4-5e89-4888-9f05-45e5e181d27f%2FUntitled.png?table=block&id=9300df07-51a3-4c32-805f-bd88c8e6c9e1&spaceId=effd33e2-527b-43dc-aef5-7b5ee7b83428&width=2000&userId=&cache=v2" alt="img" style="zoom: 67%;" />

- Update：每次更新一次参数叫做一次 Update,
- Epoch：把所有的 Batch 都看过一遍,叫做一个 Epoch

模型变型⇒ReLU（Rectified Linear Unit，线性整流单元）

- 把两个 ReLU 叠起来,就可以变成 Hard 的 Sigmoid



# 02.1-DeepLearning-General Guidance

![img](https://diamond-mule-bee.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F6586a506-6ac8-43ad-a1d9-28e6de0b1fdb%2FUntitled.png?table=block&id=53a2d275-3157-4673-93d0-c137a8465213&spaceId=effd33e2-527b-43dc-aef5-7b5ee7b83428&width=2000&userId=&cache=v2)

##  如何使模型达到更好的效果？

1. 分析在训练数据上的Loss

   - Model Bias

     - 所有的function集合起来,得到一个function的set.但是这个function的set太小了,没有包含任何一个function,可以让我们的loss变低⇒**可以让loss变低的function,不在model可以描述的范围内。**

       ⇒解决方法：重新设计一个Model，**一个更复杂的、更有弹性的、有未知参数的、需要更多features的function

   - Optimization

   - 区分两种情况

     - 看到一个你从来没有做过的问题,也许你可以先跑一些比较小的,比较浅的network,或甚至用一些,不是deep learning的方法⇒比较容易做Optimize的,它们比较不会有optimization失败的问题
     - 如果你发现你深的model,跟浅的model比起来,深的model明明弹性比较大,但loss却没有办法比浅的model压得更低,那就代表说你的optimization有问题

2. 分析测试数据上的Loss

   - Overfitting：training的loss小,testing的loss大,这个有可能是overfitting

     如果你的model它的**自由度很大**的话,它可以**产生非常奇怪的曲线**,导致训练集上的结果好,但是测试集上的loss很大

   - 解决方法

     - 增加训练集
     - 限制模型，使其弹性不那么大
       - 给它**比较少的参数（比如神经元的数目）；模型**共用参数**
       - 使用**比较少的features**
       - Early Stopping
       - Regularization
       - Dropout

==选出有较低testing-loss的模型==

- Cross Validation（交叉验证）

<img src="https://diamond-mule-bee.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F5782b3e5-2e99-488f-ba63-aaec6b59631d%2FUntitled.png?table=block&id=89a42d1a-8be3-41f5-923a-95f55307fb0d&spaceId=effd33e2-527b-43dc-aef5-7b5ee7b83428&width=2000&userId=&cache=v2" alt="img" style="zoom:50%;" />

1. 把Training的资料分成两半,一部分叫作Training Set,一部分是Validation Set
2. 在Validation Set上面,去衡量它们的分数,你根据Validation Set上面的分数,去挑选结果，不要管在public testing set上的结果，避免overfiting（过拟合）

==分验证集==

- N-fold Cross Validation （N倍交叉验证）

  **N-fold Cross Validation**就是你先把你的训练集切成N等份,在这个例子里面我们切成N等份,切完以后,你拿其中一份当作Validation Set,另外N-1份当Training Set,重复N次

  把这多个模型,在这三个setting下,通通跑过一次,把三种状况的结果都平均起来,看看谁的结果最好；最后再把选出来的model（这里是model 1）,用在全部的Training Set上,训练出来的模型,再用在Testing Set上面



# 02.2-类神经网络优化技巧

## critical point 概述

==临界点==：梯度 （grad） 为 0 的点

loss,无法再下降,也许是因为卡在了critical point ⇒ local minima（局部最小值） OR saddle point（鞍点）

- 局部最小值 -> 可能无路可走
- 鞍点 -> 旁边还是有路可走

`判断θ附近loss的梯度 -> 泰勒展开 -> 海塞矩阵`

<img src="https://diamond-mule-bee.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fcad56cb3-6fc6-4488-b1cd-b567fea2075c%2FUntitled.png?table=block&id=1038a41a-a019-41ec-9e08-2e9a062e9bfc&spaceId=effd33e2-527b-43dc-aef5-7b5ee7b83428&width=2000&userId=&cache=v2" alt="img" style="zoom:50%;" />

<img src="https://diamond-mule-bee.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F515415e7-dcaa-499c-8dcb-9268061f824e%2FUntitled.png?table=block&id=5d24c4da-36ca-43f5-9ea0-1c8935326271&spaceId=effd33e2-527b-43dc-aef5-7b5ee7b83428&width=2000&userId=&cache=v2" alt="img" style="zoom:50%;" />

- g：一次微分，当到达临界点时，表示g为0，则绿色项不存在，原式等于第一项加红色项

- H：二次微分，根据红色项来判断临界点为哪种情况

<img src="https://diamond-mule-bee.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fc88a449d-25e8-4c35-9e3f-5afe3e46e7b0%2FUntitled.png?table=block&id=15c6ddd8-63da-45eb-8499-085c395e3542&spaceId=effd33e2-527b-43dc-aef5-7b5ee7b83428&width=2000&userId=&cache=v2" alt="img" style="zoom: 67%;" />

- 红色部分 大于0，左边永远大于右边 L(θ`)，所以该点为最小值点
- 红色部分 小于0，左边永远小于右边 L(θ`)，所以该点为最大值点
- 红色部分有时大于0有时小于0，则为鞍点

> 注：如果走到鞍点，可以利用H的特征向量确定参数的更新方向
>
> 令特征值小于0，得到对应的特征向量u,在θ`的位置加上u,沿著u的方向做update得到θ,就可以让loss变小。
>
> > Local Minima比Saddle Point少的多

###  Small Batch v.s. Large Batch

`batch_size 取大取小的关系：`

- 结论1：使用较小的BatchSize，在更新参数时会有Noisy（图中曲线弯弯折折）⇒有利于训练
  
  - 不同的Batch 求得的Loss略有差异，可以避免局部极小值“卡住”
  
  ![img](https://diamond-mule-bee.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F9ec2c77d-56dd-4f99-8efc-782ffa48586f%2FUntitled.png?table=block&id=b876aaa1-e583-4960-997f-2a5d62968d22&spaceId=effd33e2-527b-43dc-aef5-7b5ee7b83428&width=2000&userId=&cache=v2)

- 结论2：使用较小的BatchSize,可以避免Overfitting ⇒ 有利于测试(Testing)
  
  - SB 在数据集（测试集）表现更好
  
  <img src="https://diamond-mule-bee.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F149f4354-b2a9-41ee-bf78-a3e96d1ca042%2FUntitled.png?table=block&id=91fc00ba-88ff-4e1b-bcf4-2512b8047204&spaceId=effd33e2-527b-43dc-aef5-7b5ee7b83428&width=2000&userId=&cache=v2" alt="img" style="zoom: 80%;" />
  
- 总结：BatchSize是一个需要调整的参数，它会影响训练速度与优化效果。

  <img src="https://diamond-mule-bee.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F3158ec11-5567-4ef4-a7d6-79d3db676675%2FUntitled.png?table=block&id=6160f7de-7bcc-4299-8e1d-39cf98246302&spaceId=effd33e2-527b-43dc-aef5-7b5ee7b83428&width=1090&userId=&cache=v2" alt="img" style="zoom: 80%;" />

### Momentum（动量）

> 所谓的 Momentum, Update 的方向不是只考虑现在的 Gradient,而是考虑过去所有 Gradient 的总合

- Vanilla Gradient Descent（一般的梯度下降）⇒ 只考虑梯度的方向，向反方向移动
- Gradient Descent + Momentum（考虑动量）⇒ 综合梯度+前一步的方向

### 总结 - 关于“小梯度”

- 临界点的梯度为 0 
- 临界值点可以是鞍点或者局部最小值点
  - 可以通过海塞矩阵确定
  - 局部最小值可能很少见
  - 有可能沿着海塞矩阵的特征向量的方向逃出鞍点
- 较小的批次规模和动量有利于避开临界点

### 自适应学习率

`Error Surface`: 根据不同参数，计算Loss得到的等高线图

- Training  stuck 训练卡住或暂停不一定是最小梯度 -> Loss不再下降时，未必说明到达临界点，梯度可能还很大

<img src="https://diamond-mule-bee.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb0915a78-64fd-4120-87b1-2a03ca9f3318%2FUntitled.png?table=block&id=6c742f56-dd7f-4176-b101-1aedf57fe0ab&spaceId=effd33e2-527b-43dc-aef5-7b5ee7b83428&width=2000&userId=&cache=v2" alt="img" style="zoom:67%;" />

- 客制化“学习率”
  - 较大的学习率：Loss在山谷的两端震荡而不会下降
  - 较小的学习率：梯度较小时几乎难以移动

> 客制化“梯度” ⇒  不同的参数（大小）需要不同的学习率
>
> > **基本原则：**
> >
> > - 某一个方向上gradient的值很小,非常的平坦⇒learning rate调大一点,
> > - 某一个方向上非常的陡峭,坡度很大⇒learning rate可以设得小一点

`Adagrad`: 每次更新的𝜂就是等于前一次的𝜂再除以𝜎^t，而 σ^t则代表的是第 t 次以前的所有梯度更新值之平方和开根号(root mean square)。

- gradient都比较大,σ就比较大,在update的时候 参数update的量就比较小。
  - 缺陷 ⇒ 不能“实时”考虑梯度的变化情况

`RMSProp`:

添加参数$\alpha$，越大说明过去的梯度信息**更重要**

- α设很小趋近於0,就代表这一步算出的gᵢ相较於之前所算出来的gradient而言比较重要
- α设很大趋近於1,就代表现在算出来的gᵢ比较不重要,之前算出来的gradient比较重要

`最常用的策略：Adam=RMSProp + Momentum`

​	Adam其实就是加了momentum的RMSProp，下图的公式mt代表的是momentum，就是前一个时间点的movement（momentum动量），vt就是RMSProp裡的σ，式子虽然看起來很复杂，但其實跟RMSProp很類似，

​	每次更新都會调整新的gradient的比重。所以**Adam**继承两者的优点，适合大部分的状况，为目前最常使用的优化方法。

<img src="https://miro.medium.com/max/1400/1*EmEGVj6OvV0kEdsT-nyIlg.png" alt="img" style="zoom:50%;" />![img](https://miro.medium.com/max/780/1*_oz4zR8-sUB6a7lf3j42Bw.png)

> Learning Rate Decay：随著时间的不断地进行,随著参数不断的update,η越来越小

`Warm Up⇒让learning rate先变大后变小`

- 在一开始，**σ^t**下标 **i **的估计值有很大的方差

- σ指示某一个方向它到底有多陡/多平滑,这个统计的结果,要看得够多笔数据以后才精准,所以一开始我们的统计是不精準的。一开始learning rate比较小，是让它探索收集一些有关error surface的情报，在这一阶段使用较小的learning rate，限制参数不会走的离初始的地方太远；等到σ统计得比较精準以后再让learning rate慢慢爬升

### 总结

1. 使用动量，考虑过去的梯度**“大小”与“方向”**
2. 引入$\sigma$,考虑过去梯度的“**大小**”（RMS）
3. 使用LearningRate Schedule

### 另一种思路

- 将Error Surface“铲平”  ⇒  Batch Normalization 批次标准化（归一化）
- **解决：给不同的 dimension同样的数值范围  ⇒  Feature Normalization(归一化)**
  - 一种Standardization（标准化）方法：对不同数据样本向量的**同一维**进行归一化
  - 做完 normalize 以后,这个 dimension 上面的数值就会平均是 0,然后它的 variance就会是 1,所以**这一排数值的分布就都会在 0 上下**
  - 对每一个 dimension都做一样的 normalization,就会发现所有 feature 不同 dimension 的数值都在 0 上下,那你可能就可以**制造一个,比较好的 error surface**
- 在深度学习中，每一层都需要一次Normalization
  - 对向量的对应element做求平均、标准差的运算，求得向量$\mu,\sigma$
  - 对每个向量$z$,利用$\mu,\sigma$对对应element进行归一化，得到$\tilde{z}$
  - 继续后续的步骤

> 注意理解：“对一批z数据进行归一化”  ⇒  “网络”模型变为能够一次处理“一批x数据”的模型，数据之间相互关联
>
> Batch Normalization：实际上做Normalization时，只能考虑有限数量的数据⇒考虑一个Batch内的数据⇒近似整个数据集
>
> > Batch Normalization适用於 batch size 比较大时。其中data可以认为足以表示整个 corpus 的分布；从而，将对整个 corpus做 Feature Normalization 这件事情,改成只在一个 batch中做 Feature Normalization作为近似



# 02.3-DeepLearning-Loss of Classification

> 各种损失函数的建议

## Classification as Regression

`Classification with softmax`

![img](https://diamond-mule-bee.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F96fb2865-7d3f-42ea-acee-6da18cdd6260%2FUntitled.png?table=block&id=60e4d4be-6caf-4908-8b47-02633b3d4976&spaceId=effd33e2-527b-43dc-aef5-7b5ee7b83428&width=2000&userId=&cache=v2)

我们的目标只有0跟1,而y有任何值,我们就使用Softmax先把它Normalize到0到1之间,这样才好跟 label 的计算相似度

<img src="https://diamond-mule-bee.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ff52d8d7e-0ba4-4e96-8109-6eb741ec3a2d%2FUntitled.png?table=block&id=15c46703-192d-46ae-9e96-4918fd19edcb&spaceId=effd33e2-527b-43dc-aef5-7b5ee7b83428&width=2000&userId=&cache=v2" alt="img" style="zoom:67%;" />

经过计算后：

- 输出值变成0到1之间
- 输出值的和為1
- 原本大的值跟小的值的**差距更大**

Softmax的输入，称作**Logit**

> 二分类：使用sigmoid与softmax是等价的

## 优化目标：减小y^和y'之间的差距e

- 在分类问题上，交叉熵相比较 MSE 更加适合

- > 注：在Pytorch中，softmax 函数被内建在Cross-enrtopy 损失函数中

- 从优化角度出发进行讨论，使用MSE时，左上角的位置虽然Loss很大，但梯度平坦，难以优化；而Cross-entropy则更易收敛⇒改变Loss函数，也会影响训练的过程

<img src="https://diamond-mule-bee.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F1f6dae63-ffbc-41c6-83ee-bcf286ef2764%2FUntitled.png?table=block&id=e028b452-d68c-4a12-be9e-42962d24b75a&spaceId=effd33e2-527b-43dc-aef5-7b5ee7b83428&width=2000&userId=&cache=v2" alt="img" style="zoom: 80%;" />



# 03-CNN

==网络模型的架构设计思想==

## 场景： 图片分类

- 一般过程：
  - 把所有图片都先 Rescale 成大小一样
  - 把每一个类别,表示成一个 One-Hot 的 Vector（Dimension 的长度就决定了模型可以辨识出多少不同种类的东西,）
  - 将图像【输入】到模型中
- 如何将图片输入到模型中？⇒  一般思路：展平→参数量过大

## 神经元角度

`观察（1）：模型通过识别一些“特定模式”来识别物体，而非“整张图”`

`简化（1）：设定“感受野”（Receptive Field）`

- 每个神经元只需要考察特定范围内的图像信息，将图像内容展平后输入到神经元中即可

  - 感受野之间可以重叠
  - 一个感受野可以有多个神经元“守备”
  - 感受野大小可以“有大有小”
  - 感受野可以只考虑某一些Channel
  - 感受野可以是“长方形”的
  - 感受野不一定要“相连”

- 感受野的基本设置

  - 看所有的Channel

    一般在做影像辨识的时候会看全部的 Channel。那么，在描述一个 Receptive Field 的时候,无需说明其Channel数，只要讲它的**高、宽⇒Kernel Size**

    一般不做过大的kernal Size，**常常设定为**$3\times3$

  - 每个感受野会有**不止一个神经元进行“守备”⇒输出通道数/卷积核数目**

  - 不同的感受野之间的关系⇒感受野的平移位移：stride【hyperparameter】

    一般希望感受野之间有重叠，避免交界处的pattern被忽略

  - 感受野超出影响的范围⇒padding（补值）

    补0；补平均值；补边缘值……

  - 垂直方向移动

`观察（2）：同样的pattern，可能出现在图片的“不同位置”`

`简化（2）：不同 Receptive Field 的 Neuron共享参数⇒ Parameter Sharing权值共享`

- 对每个感受野，都使用一组相同的神经元进行守备；这一组神经元被称作filter，对不同感受野使用的filter参数相同



### 卷积层的优势

> 卷积层是“受限”（弹性变小）的FC（全连接层）

<img src="https://diamond-mule-bee.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ff84607df-a9c7-4df9-a346-6e64b748e790%2FUntitled.png?table=block&id=32d002ac-b6d6-46b1-8658-e6a1b1ede39c&spaceId=effd33e2-527b-43dc-aef5-7b5ee7b83428&width=2000&userId=&cache=v2" alt="img" style="zoom:67%;" />

- FC可以通过“学习”决定要看到的“图片”的范围。加上“感受野”概念后，就只能看某一个范围。
- FC可以自由决定守备不同“感受野”的各个神经元参数。加上“权值共享”概念后，守备不同感受野的**同一个滤波器（filter）参数相同。**

**分析：**

- 一般而言，Model Bias 小,Model 的 Flexibility 很高的时候,它比较容易 Overfitting,Fully Connected Layer可以做各式各样的事情,它**可以有各式各样的变化**,但是它可能没有办法在任何**特定的任务**上做好
- CNN 的 Bias 比较大，它是专门為影像设计的，所以它在影像上仍然可以做得好。

## 滤波器角度

卷积层中有若干个filter，每个filter可以用来“抓取”图片中的某一种特征（特征pattern的大小，小于感受野大小）。

ilter的参数，其实就是神经元中的“权值（weight）”。

不同的filter扫过一张图片，将会产生“新的图片”，每个filter将会产生图片中的一个channel⇒**feature map**

filter的计算是**“内积”**：filter跟图片对应位置的数值直接相乘，所有的都乘完以后再相加。



`多层卷积`

- 多层卷积⇒让“小”卷积核看到“大”pattern

## 总结

两个角度理解“卷积”：神经元角度（neuron）与滤波器（filter）

|        |    不用看整张图片范围     | 图片不同位置的相同模式pattern  |
| :----: | :-----------------------: | :----------------------------: |
| 神经元 |      只要守备感受野       | 守备不同感受野的神经元共用参数 |
| 滤波器 | 使用滤波器侦测模式pattern |      滤波器“扫过”整张图片      |

## 观察（3）Pooling

==图片降采样不影响图片的辨析⇒Pooling（池化）把图片变小，减小运算量==

> Pooling本身没有参数,所以它不是一个 Layer，没有要 Learn 的东西。行为类似于一个 Activation Function,（Sigmoid ， ReLU ），是一个 Operator，行为固定
>
> 分类：
>
> - Max pooling
> - Avg Pooling

## The whole CNN（典型分类网络结构）

conv-pooling-...（循环）-flatten-FC-softmax

- 一般：卷积与池化交替使用。pooling⇒可有可无（算力支撑）

- 算力足够 ->  无需池化
  Pooling对于 Performance,会带来一点伤害的。如果你运算资源足够支撑你不做 Pooling 的话,很多 Network 的架构的设计,往往今天就不做 Pooling,全 Convolution。

> 最后注意：
>
> CNN 并不能够处理影像放大缩小,或者是旋转的问题。所以在做影像辨识的时候,往往都要做 Data Augmentation（数据增强），把你的训练数据截一小块出来放大缩小、把图片旋转,CNN 才会做到好的结果。
>
> 有一个架构叫 spacial Transformer Layer可以处理。