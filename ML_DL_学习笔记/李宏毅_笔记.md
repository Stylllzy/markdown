# 01-Regression

##  Machine Learning

> 概况：机器学习就是让机器具备找一个函式的能力。

##  三个机器学习任务

- Regression：要找的函式,他的输出是一个数值
- Classification:函式的输出,就是从设定好的选项裡面,选择一个当作输出
- Structured Learning:机器产生有结构的东西的问题——学会创造

## 找函式的过程：三个步骤

1. 带有未知参数的函式

   - 带有Unknown的Parameter的Function => model

2. 定义一个东西叫做Loss(损失函数)

   - Loss它也是一个Function,那这个Function它的输入,是 我们Model裡面的参数
     - L越大,代表一组参数越不好,这个大L越小,代表现在这一组参数越好
     - 计算方法：求取估测的值跟实际的值（Label） 之间的差距
       - MAE(mean absolute error)——平均绝对误差
       - MSE(mean square error)——均方误差
       - Cross-entropy:计算**概率分布**之间的差距——交叉熵

3. Optimization(优化)

   - 找到能让损失函数值最小的参数

   - 具体方法：Gradient Descent（梯度下降）

     1. 随机选取初始值 $w_0$

     2. 计算在 $w=w_0$的时候,*w*这个参数对*loss*的微分是多少

     3. 根据微分（梯度）的方向，改变参数的值

        - **改变的大小取决于：**
          1. 斜率的大小
          2. 学习率的大小**（超参数）**

     4. 什么时候停下来？

        - 自己设置上限**（超参数）**

        - 理想情况：微分值为0（极小值点），不会再更新⇒有可能陷入局部最小值，不能找到全局最小值

          **事实上：局部最小值不是真正的问题！！！**

##  Liner Model（线性模型）

==Sigmoid函数==

```markdown
Sigmoid：y = c\frac{1}{{1+e^{-(b+wx_1)}}}
```

- Update：每次更新一次参数叫做一次 Update,

- Epoch：把所有的 Batch 都看过一遍,叫做一个 Epoch

模型变型⇒ReLU（Rectified Linear Unit，线性整流单元）

- 把两个 ReLU 叠起来,就可以变成 Hard 的 Sigmoid



# 02.1-DeepLearning-General Guidance

![img](https://diamond-mule-bee.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F6586a506-6ac8-43ad-a1d9-28e6de0b1fdb%2FUntitled.png?table=block&id=53a2d275-3157-4673-93d0-c137a8465213&spaceId=effd33e2-527b-43dc-aef5-7b5ee7b83428&width=2000&userId=&cache=v2)

##  如何使模型达到更好的效果？

1. 分析在训练数据上的Loss

   - Model Bias

     - 所有的function集合起来,得到一个function的set.但是这个function的set太小了,没有包含任何一个function,可以让我们的loss变低⇒**可以让loss变低的function,不在model可以描述的范围内。**

       ⇒解决方法：重新设计一个Model，**一个更复杂的、更有弹性的、有未知参数的、需要更多features的function

   - Optimization

   - 区分两种情况

     - 看到一个你从来没有做过的问题,也许你可以先跑一些比较小的,比较浅的network,或甚至用一些,不是deep learning的方法⇒比较容易做Optimize的,它们比较不会有optimization失败的问题
     - 如果你发现你深的model,跟浅的model比起来,深的model明明弹性比较大,但loss却没有办法比浅的model压得更低,那就代表说你的optimization有问题

2. 分析测试数据上的Loss

   - Overfitting：training的loss小,testing的loss大,这个有可能是overfitting

     如果你的model它的**自由度很大**的话,它可以**产生非常奇怪的曲线**,导致训练集上的结果好,但是测试集上的loss很大

   - 解决方法

     - 增加训练集
     - 限制模型，使其弹性不那么大
       - 给它**比较少的参数（比如神经元的数目）；模型**共用参数**
       - 使用**比较少的features**
       - Early Stopping
       - Regularization
       - Dropout

==选出有较低testing-loss的模型==

- Cross Validation	（交叉验证）

<img src="https://diamond-mule-bee.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F5782b3e5-2e99-488f-ba63-aaec6b59631d%2FUntitled.png?table=block&id=89a42d1a-8be3-41f5-923a-95f55307fb0d&spaceId=effd33e2-527b-43dc-aef5-7b5ee7b83428&width=2000&userId=&cache=v2" alt="img" style="zoom:50%;" />

1. 把Training的资料分成两半,一部分叫作Training Set,一部分是Validation Set
2. 在Validation Set上面,去衡量它们的分数,你根据Validation Set上面的分数,去挑选结果，不要管在public testing set上的结果，避免overfiting（过拟合）

==分验证集==

- N-fold Cross Validation	（N倍交叉验证）

  **N-fold Cross Validation**就是你先把你的训练集切成N等份,在这个例子里面我们切成N等份,切完以后,你拿其中一份当作Validation Set,另外N-1份当Training Set,重复N次

  把这多个模型,在这三个setting下,通通跑过一次,把三种状况的结果都平均起来,看看谁的结果最好；最后再把选出来的model（这里是model 1）,用在全部的Training Set上,训练出来的模型,再用在Testing Set上面



# 02.2-类神经网络优化技巧

## critical point 概述

==临界点==：梯度 （grad） 为 0 的点

loss,无法再下降,也许是因为卡在了critical point ⇒ local minima（局部最小值） OR saddle point（鞍点）

- 局部最小值 -> 可能无路可走
- 鞍点 -> 旁边还是有路可走

`判断θ附近loss的梯度 -> 泰勒展开 -> 海塞矩阵`

<img src="https://diamond-mule-bee.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fcad56cb3-6fc6-4488-b1cd-b567fea2075c%2FUntitled.png?table=block&id=1038a41a-a019-41ec-9e08-2e9a062e9bfc&spaceId=effd33e2-527b-43dc-aef5-7b5ee7b83428&width=2000&userId=&cache=v2" alt="img" style="zoom:50%;" />

<img src="https://diamond-mule-bee.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F515415e7-dcaa-499c-8dcb-9268061f824e%2FUntitled.png?table=block&id=5d24c4da-36ca-43f5-9ea0-1c8935326271&spaceId=effd33e2-527b-43dc-aef5-7b5ee7b83428&width=2000&userId=&cache=v2" alt="img" style="zoom:50%;" />

- g：一次微分，当到达临界点时，表示g为0，则绿色项不存在，原式等于第一项加红色项

- H：二次微分，根据红色项来判断临界点为哪种情况

<img src="https://diamond-mule-bee.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fc88a449d-25e8-4c35-9e3f-5afe3e46e7b0%2FUntitled.png?table=block&id=15c6ddd8-63da-45eb-8499-085c395e3542&spaceId=effd33e2-527b-43dc-aef5-7b5ee7b83428&width=2000&userId=&cache=v2" alt="img" style="zoom: 67%;" />

- 红色部分 大于0，左边永远大于右边 L(θ`)，所以该点为最小值点
- 红色部分 小于0，左边永远小于右边 L(θ`)，所以该点为最大值点
- 红色部分有时大于0有时小于0，则为鞍点

> 注：如果走到鞍点，可以利用H的特征向量确定参数的更新方向
>
> 令特征值小于0，得到对应的特征向量u,在θ`的位置加上u,沿著u的方向做update得到θ,就可以让loss变小。
>
> > Local Minima比Saddle Point少的多

###  `Small Batch v.s. Large Batch`

`batch_size 取大取小的关系：`

- 结论1：使用较小的BatchSize，在更新参数时会有Noisy（图中曲线弯弯折折）⇒有利于训练
  
  - 不同的Batch 求得的Loss略有差异，可以避免局部极小值“卡住”
  
  ![img](https://diamond-mule-bee.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F9ec2c77d-56dd-4f99-8efc-782ffa48586f%2FUntitled.png?table=block&id=b876aaa1-e583-4960-997f-2a5d62968d22&spaceId=effd33e2-527b-43dc-aef5-7b5ee7b83428&width=2000&userId=&cache=v2)

- 结论2：使用较小的BatchSize,可以避免Overfitting ⇒ 有利于测试(Testing)
  
  - SB 在数据集（测试集）表现更好
  
  <img src="https://diamond-mule-bee.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F149f4354-b2a9-41ee-bf78-a3e96d1ca042%2FUntitled.png?table=block&id=91fc00ba-88ff-4e1b-bcf4-2512b8047204&spaceId=effd33e2-527b-43dc-aef5-7b5ee7b83428&width=2000&userId=&cache=v2" alt="img" style="zoom: 80%;" />
  
- 总结：BatchSize是一个需要调整的参数，它会影响训练速度与优化效果。

  <img src="https://diamond-mule-bee.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F3158ec11-5567-4ef4-a7d6-79d3db676675%2FUntitled.png?table=block&id=6160f7de-7bcc-4299-8e1d-39cf98246302&spaceId=effd33e2-527b-43dc-aef5-7b5ee7b83428&width=1090&userId=&cache=v2" alt="img" style="zoom: 80%;" />

### `Momentum（动量）`

> 所谓的 Momentum, Update 的方向不是只考虑现在的 Gradient,而是考虑过去所有 Gradient 的总合

- Vanilla Gradient Descent（一般的梯度下降）⇒ 只考虑梯度的方向，向反方向移动
- Gradient Descent + Momentum（考虑动量）⇒ 综合梯度+前一步的方向

### `总结 - 关于“小梯度”`

- 临界点的梯度为 0 
- 临界值点可以是鞍点或者局部最小值点
  - 